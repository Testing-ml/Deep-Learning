{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression\n",
    "\n",
    "Also known as, Ordinary least squares Linear Regression.\n",
    "\n",
    "LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Importing \n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Features\n",
    "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "# y = 1 * x_0 + 2 * x_1 + 3 -> Target\n",
    "y = np.dot(X, np.array([1, 2])) + 3\n",
    "# Fit method\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "# Return the coefficient of determination R^2 of the prediction.\n",
    "print(reg.score(X, y))\n",
    "# Return predicted value of y for X (array-like) using the linear model. \n",
    "print(reg.predict(np.array([[3, 5]]))) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0\n",
      "[16.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Alos know as logit & MaxEnt classifier.\n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’. (Currently the ‘multinomial’ option is supported only by the ‘lbfgs’, ‘sag’, ‘saga’ and ‘newton-cg’ solvers.)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Importing\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the data\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# Fit the model\n",
    "clf = LogisticRegression(random_state=0, max_iter=10000000).fit(X, y)\n",
    "# Predict\n",
    "print(clf.predict(X[:2, :]))\n",
    "# Predict class probabilities\n",
    "print(clf.predict_proba(X[:2, :]))\n",
    "# Classifier Score\n",
    "print(clf.score(X, y))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 0]\n",
      "[[9.81585270e-01 1.84147159e-02 1.45076469e-08]\n",
      " [9.71333598e-01 2.86663719e-02 3.02076222e-08]]\n",
      "0.9733333333333334\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision Tree\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Importing\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Classifier object\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "# Load the data -> contains the features and the target\n",
    "iris = load_iris()\n",
    "# Cross-validation score\n",
    "print(cross_val_score(clf, iris.data, iris.target, cv=10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.         0.93333333 1.         0.93333333 0.93333333 0.86666667\n",
      " 0.93333333 1.         1.         1.        ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest \n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Importing \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "# Classifier object\n",
    "clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "# Fit the model\n",
    "clf.fit(X, y)\n",
    "# Score\n",
    "print(clf.score(X, y))\n",
    "# Predict\n",
    "print(clf.predict([[0, 0, 0, 0]]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.946\n",
      "[1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SVM\n",
    "\n",
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "- Effective in high dimensional spaces.\n",
    "- Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "- Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Importing\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "# Features\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "# Targets\n",
    "y = np.array([1, 1, 2, 2])\n",
    "# Pipeline\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "# Fit the model\n",
    "clf.fit(X, y)\n",
    "# Classifier score\n",
    "clf.score(X, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('ml': conda)"
  },
  "interpreter": {
   "hash": "e0289065fd70ee0301126581a02cae4c59a3b78f12d586ce6d502aba513ac9a8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}